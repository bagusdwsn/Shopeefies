{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "019f45df-a38f-4440-8123-3b32085f74cd",
   "metadata": {
    "id": "019f45df-a38f-4440-8123-3b32085f74cd"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "QjJ08DV53tjU",
   "metadata": {
    "id": "QjJ08DV53tjU"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a660fb19-8589-493d-ba55-7d6fc05f1b6f",
   "metadata": {
    "id": "a660fb19-8589-493d-ba55-7d6fc05f1b6f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv('data/gamis1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f877bff-db7b-4b03-ab7c-b6dcdc77f5d1",
   "metadata": {
    "id": "1f877bff-db7b-4b03-ab7c-b6dcdc77f5d1"
   },
   "outputs": [],
   "source": [
    "# Define sentiment thresholds\n",
    "positive_threshold = 4\n",
    "negative_threshold = 2\n",
    "\n",
    "# Create a new column for sentiment labels\n",
    "dataset['sentimen'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcabfe1-0b50-43a4-b2fb-50d96e9fec73",
   "metadata": {
    "id": "2bcabfe1-0b50-43a4-b2fb-50d96e9fec73"
   },
   "outputs": [],
   "source": [
    "# Iterate through the dataset and assign sentiment labels\n",
    "for index, row in dataset.iterrows():\n",
    "    rating = row['rating']\n",
    "    \n",
    "    if rating >= positive_threshold:\n",
    "        dataset.at[index, 'sentimen'] = 'positif'\n",
    "    elif rating <= negative_threshold:\n",
    "        dataset.at[index, 'sentimen'] = 'negatif'\n",
    "    else:\n",
    "        dataset.at[index, 'sentimen'] = 'netral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "882ec112-35ff-45a3-8cb4-ba555fda8798",
   "metadata": {
    "id": "882ec112-35ff-45a3-8cb4-ba555fda8798"
   },
   "outputs": [],
   "source": [
    "# Save the updated dataset\n",
    "dataset.to_csv('labeled_ds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c2f352a-16d8-4ae0-8eb0-3a019aa3a948",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6c2f352a-16d8-4ae0-8eb0-3a019aa3a948",
    "outputId": "17e6ea6d-d0fd-46ac-d4aa-9fb74b2be647"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>makasih sudah sampai paketan nya ...mas kurir ...</td>\n",
       "      <td>5</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barang nya cpt nympe nya,bgs.\\nmksh lazada.</td>\n",
       "      <td>5</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bahannya bagus. baju nya pas di pakai üëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alhmdllh brng udh nympe...alhmdllh jg bju ssua...</td>\n",
       "      <td>5</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bajunya cantik walaupun warnanya aga' beda den...</td>\n",
       "      <td>5</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating sentimen\n",
       "0  makasih sudah sampai paketan nya ...mas kurir ...       5  positif\n",
       "1        Barang nya cpt nympe nya,bgs.\\nmksh lazada.       5  positif\n",
       "2            bahannya bagus. baju nya pas di pakai üëç       5  positif\n",
       "3  alhmdllh brng udh nympe...alhmdllh jg bju ssua...       5  positif\n",
       "4  bajunya cantik walaupun warnanya aga' beda den...       5  positif"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02466ec-d183-4923-bb33-396474f2f998",
   "metadata": {
    "id": "b02466ec-d183-4923-bb33-396474f2f998"
   },
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gW4y0YFy46td",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gW4y0YFy46td",
    "outputId": "23268b15-5189-421a-8d09-28167f295b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sastrawi in c:\\users\\lenovo\\anaconda3\\envs\\calma\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sastrawi\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4DQyo0Uf5WlA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DQyo0Uf5WlA",
    "outputId": "8fa8a4c2-14ff-4958-e66f-f64e179f33e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "530b6457-22d6-445a-9bf6-4b05327db45e",
   "metadata": {
    "id": "530b6457-22d6-445a-9bf6-4b05327db45e"
   },
   "outputs": [],
   "source": [
    "def normalize_review(review):\n",
    "    # Clean the text\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(review))\n",
    "    review = review.lower()  # Convert to lowercase\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(review)\n",
    "\n",
    "    # Initialize Sastrawi stemmer and stopword remover\n",
    "    stemmer = StemmerFactory().create_stemmer()\n",
    "    stopwords = StopWordRemoverFactory().get_stop_words()\n",
    "\n",
    "    # Normalize each token\n",
    "    normalized_tokens = []\n",
    "    for token in tokens:\n",
    "        # Remove stopwords\n",
    "        if token not in stopwords:\n",
    "            # Stemming\n",
    "            stemmed_token = stemmer.stem(token)\n",
    "            normalized_tokens.append(stemmed_token)\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    normalized_review = ' '.join(normalized_tokens)\n",
    "    \n",
    "    return normalized_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169753bc-7e67-4878-84e1-0cf9a38e8f8f",
   "metadata": {
    "id": "169753bc-7e67-4878-84e1-0cf9a38e8f8f"
   },
   "outputs": [],
   "source": [
    "dataset['review'] = dataset['review'].apply(normalize_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d383f11-17cc-4e64-9d88-d671190ab436",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9d383f11-17cc-4e64-9d88-d671190ab436",
    "outputId": "b77ba8db-6d9f-498e-bbf3-36788dcfa4d0"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jr_wlwQ58QTT",
   "metadata": {
    "id": "jr_wlwQ58QTT"
   },
   "outputs": [],
   "source": [
    "# Save the updated dataset\n",
    "dataset.to_csv('data/preprocessed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dVdv1pkT9igR",
   "metadata": {
    "id": "dVdv1pkT9igR"
   },
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RigSt4Um9nJq",
   "metadata": {
    "id": "RigSt4Um9nJq"
   },
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wuFFs7sw9mUU",
   "metadata": {
    "id": "wuFFs7sw9mUU"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = dataset['review']  # Input features (normalized reviews)\n",
    "y = dataset['sentimen']  # Target variable (sentiment labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GU0xIrIW9phX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GU0xIrIW9phX",
    "outputId": "a03db668-eb88-4f80-c919-31f9abd9194a"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train a Word2Vec model on your corpus (X_train)\n",
    "embedding_dim = 100  # Adjust the embedding dimension based on your requirements\n",
    "embedding_model = Word2Vec(sentences=X_train, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get word embeddings for your vocabulary\n",
    "word_vectors = embedding_model.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7QfHDKdA-tBV",
   "metadata": {
    "id": "7QfHDKdA-tBV"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Create a tokenizer to convert words to tokens\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences of tokens\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_seq_length = 100  # Adjust the sequence length based on your data\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_seq_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Convert sentiment labels to numerical values\n",
    "sentiment_mapping = {'negatif': 0, 'netral': 1, 'positif': 2}\n",
    "y_train_numerical = [sentiment_mapping[label] for label in y_train]\n",
    "y_test_numerical = [sentiment_mapping[label] for label in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uNtv3Bpi-K1M",
   "metadata": {
    "id": "uNtv3Bpi-K1M"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Build the sentiment analysis model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_seq_length))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PpJ4lm5Y_IUu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PpJ4lm5Y_IUu",
    "outputId": "4e1f4262-e1b0-412a-b024-76367ec76ad4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert the input data to numpy arrays\n",
    "X_train_padded = np.array(X_train_padded)\n",
    "X_test_padded = np.array(X_test_padded)\n",
    "y_train_numerical = np.array(y_train_numerical)\n",
    "y_test_numerical = np.array(y_test_numerical)\n",
    "\n",
    "# Train the sentiment analysis model\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_train_padded, y_train_numerical, batch_size=batch_size, epochs=epochs, validation_data=(X_test_padded, y_test_numerical))\n",
    "\n",
    "# Access the training history\n",
    "print(history.history.keys())\n",
    "\n",
    "# Plot the training and validation accuracy over epochs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Training', 'Validation'])\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss over epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Training', 'Validation'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uGC8oSj-_1pe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGC8oSj-_1pe",
    "outputId": "4b051f04-6eb4-4e6f-e123-1ea30f4c6a67"
   },
   "outputs": [],
   "source": [
    "# Update the sentiment mapping dictionary to include all label indices\n",
    "#sentiment_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "# Make predictions on the training data\n",
    "train_predictions = model.predict(X_train_padded)\n",
    "train_predicted_labels = [sentiment_mapping[np.argmax(pred)] for pred in train_predictions]\n",
    "\n",
    "# Convert the true labels to sentiment labels\n",
    "y_train_labels = [sentiment_mapping[label] for label in y_train_numerical]\n",
    "\n",
    "# Calculate the accuracy\n",
    "train_accuracy = sum(np.array(train_predicted_labels) == np.array(y_train_labels)) / len(y_train_labels)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vvOOE3drA3Lk",
   "metadata": {
    "id": "vvOOE3drA3Lk"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"sentiment_analysis_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83Y2y71pElpT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "83Y2y71pElpT",
    "outputId": "98de19ab-bf58-4c22-9fb9-c1a36ff51e78"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "calma",
   "language": "python",
   "name": "calma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
